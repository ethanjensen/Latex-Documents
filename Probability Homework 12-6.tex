\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{empheq}
\usepackage{tikz}
\usetikzlibrary{automata, positioning, arrows, shapes}
\addtolength{\topmargin}{-0.875in}
\addtolength{\textheight}{1.75in}

\title{Math 331 A - Probability}
\author{Ethan Jensen}
\date{December 6th, 2019}

\begin{document}
	\maketitle HW p.208 \#3,4 \ p. 226 \#43,46 \ p. 239 \#3 \ p. 256 \#66,68,70
	\definecolor{mycolor1}{RGB}{155,155,155}
	\section[20pt]{p. 208 \#3}
	If \(X\) has the uniform density with the parameters \(\alpha=0\) and  \(\beta=1\), use the distribution function technique to find the probability density of the random variable Y = \(\sqrt{X}\). \newline \newline
	By Definition 3.3 we have
	\[F_Y(y)=P(Y<y)=P(\sqrt{X}<y)=P(X<y^2)\]
	By Def. 6.1 we have
	\[F_Y(y)=P(X<y^2)=\int_0^{y^2}dy=y^2\]
	\[f(y)=\frac{d}{dy}F_Y(y)=2y,\ 0 < y < 1.\]
	\boxed{f(y)=2y,\ \ 0<y<1}
	\newpage
	\maketitle HW p.208 \#3,4 \ p. 226 \#43,46 \ p. 239 \#3 \ p. 256 \#66,68,70
	\section[20pt]{p. 208 \#4}
	If the joint probability density of \(X\) and \(Y\) is given by
	\[f(x,y)=\left\{\begin{array}{ccc}
	4xye^{-(x^2+y^2)}\ \ \textup{for}\ x>0,y>0\\0 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textup{elsewhere} \ \ \ \ \ \ \ 
	\end{array} \right.\]
	and \(Z=\sqrt{X^2+Y^2}\), find \newline
	\textbf{(a)} the distribution function of Z; \newline
	\textbf{(b)} the probability density of Z. \newline
	\newline
	By Definition 3.3 and Definition 3.8 we have
	\[F_z(z)=P(Z<z)=P(\sqrt{X^2+Y^2}<z)\]
	\[F_z(z)=P(\sqrt{X^2+Y^2}<z)=\iint_Af(x,y)dA\]
	Below is a drawing of region A.
	\begin{figure}[ht]
		\centering
		\begin{tikzpicture}
		\def\mypath{(5,0) -- +(0,2) arc (90:0:2cm) -- +(0,0)}
		\fill [mycolor1] \mypath;
		\draw
		(5,0) edge[dashed] (5,4)
		(5,0) edge[dashed] (8,0);
		\node[below] (s3) at (7.5,0) {$x$};
		\node[below] (s5) at (8, 1.5) {radius\ $=z$};
		\node[left] (s7) at (5,3.5) {$y$};
		

		\node[] (spicy) at (6.5,4) {\textbf{A}};
		\end{tikzpicture}
		\caption{Region of integration}
	\end{figure}
	Converting the integral into polar we have
	\[F_z(z)=\int_0^{\pi/2}\int_0^{z}4\sin(\theta)\cos(\theta)r^2e^{-r^2}rdr\ d\theta\]
	This integral can be solved using Fubini's Theorem, the double angle formula for sine, a u-substitution and integration by parts. \newline
	Then a miracle happened. \newline \newline
	\boxed{\textbf{(b)}\ \ F_Z(z)=1-(1+z^2)e^{-z^2},\ 0<z<\infty} \newline \newline
	By Theorem 3.6 \newline
	\boxed{\textbf{(a)}\ \ f(z)=2z^3e^{-z^2},\ 0<z<\infty}
	\newpage
	\maketitle HW p.208 \#3,4 \ p. 226 \#43,46 \ p. 239 \#3 \ p. 256 \#66,68,70
	\section[20pt]{p. 226 \#43}
	If n independent random variables have the same gamma distribution with the parameters \(\alpha\) and \(\beta\), find the moment-generating function of their sum and, if possible, identify its distribution.
	\newline
	\newline
	Let \(Y=X_1+X_2+X_3...+X_n\) where \(X_1, X_2,...X_n\) are independent RVs with the same gamma distribution with the parameters \(\alpha\) and \(\beta\). \newline
	By Theorem 6.4 The moment-generating function of a gamma random variable \(X_i\) with \(\alpha\) and \(\beta\) is given by
	\[M_{X_i}(t)=(1-\beta t)^{-\alpha}\]
	By Theorem 7.3 we have
	\[M_Y(t)=\prod_{i=1}^nM_{X_i}(t)\]
	\[M_Y(t)=\prod_{i=1}^n(1-\beta t)^{-\alpha}\]
	\[M_Y(t)=(1-\beta t)^{-\alpha n}\]
	Variables can be uniquely identified by their moment-generating functions. \newline \newline
	\(M_Y(t)\) is a moment generating function of a gamma distribution with parameters \(\alpha=\alpha n\) and \(\beta = \beta\). \newline \newline
	\boxed{\begin{array}{ccc}
	\textbf{(a)}\ f(y) = \frac{1}{\beta^\alpha \Gamma(\alpha)}x^{n\alpha-1}e^{-x/\beta}\\
	\textbf{(b)}\ M_Y(t)=(1-\beta t)^{-\alpha n} \ \ \ \ 
	\end{array}}
	\newpage
	\maketitle HW p.208 \#3,4 \ p. 226 \#43,46 \ p. 239 \#3 \ p. 256 \#66,68,70
	\section[20pt]{p. 226 \#46}
	Use the result of Exercise 7.45 to show that, if n independent random variables \(X_i\) have normal distributions with the means \(\mu_i\) and the standard deviations \(\sigma_i\), then \(Y=a_1X_1+a_2X_2+...a_nX_n\) also has a normal distribution. What are the mean and the variance of this distribution? \newline \newline
	The moment-generating function of a normal RV \(X_i\) is given by
	\[M_{X_i}(t)=e^{\mu_i t+\frac{1}{2}\sigma_i^2t^2}\]
	Using the result of Exercise 7.45 we have
	\[M_Y(t)=\prod_{i=1}^nM_{X_i}(a_it)\]
	\[M_Y(t)=\prod_{i=1}^ne^{\mu_i t+\frac{1}{2}\sigma_i^2t^2}\]
	\[M_Y(t)=e^{(\sum_{i=1}^n\mu_i) t+\frac{1}{2}(\sum_{i=1}^n\sigma_i^2)t^2}\]
	Both \(\sum_{i=1}^n\mu_i\) and \(\sum_{i=1}^n\sigma_i^2\) are constants. \newline
	This means that \(M_Y(t)\) is the moment-generating function of a normal distribution.
	In fact those summations are the mean and variance of \(Y\) respectively! \newline \newline
	\boxed{Y\ \textup{has a normal distribution with}\  \mu_y=\sum_{i=1}^n\mu_i \ \textup{and}\  \sigma^2_y=\sum_{i=1}^n\sigma_i^2}
	\newpage
	\maketitle HW p.208 \#3,4 \ p. 226 \#43,46 \ p. 239 \#3 \ p. 256 \#66,68,70
	\section[20pt]{p. 239 \#3}
	With reference to Exercise 8.2, show that if the two samples come from normal populations, then \(\overline{X}_1-\overline{X}_2\) is a random variable having a normal distribution with the mean \(\mu_1-\mu_2\) and the variance \(\frac{\sigma^2_1}{n_1}+\frac{\sigma^2_2}{n_2}\).
	\newline \newline
	The moment-generating function for a normal distribution with mean \(\mu\) and variance \(\sigma^2\) is given by
	\[M_X(t)=e^{\mu t + \frac{1}{2}\sigma^2t^2}\]
	By Theorem 6.6 we get
	\[M_{X_1}(t)=e^{\mu_1 t + \frac{1}{2}\left(\frac{\sigma_1^2}{n_1}\right)t^2},\ \ M_{X_2}(t)=e^{\mu_2 t + \frac{1}{2}\left(\frac{\sigma_2^2}{n_2}\right)t^2}\]
	Let \(\overline{Y}=\overline{X}_1+\overline{X}_2\) be a random variable.
	By Theorem 4.10 we can write
	\[M_Y(t)=M_{\overline{X}_1}(t)M_{\overline{X}_2}(t)\]
	\[M_Y(t)=e^{\mu_1 t + \frac{1}{2}\left(\frac{\sigma_1^2}{n_1}\right)t^2}e^{\mu_2 t + \frac{1}{2}\left(\frac{\sigma_2^2}{n_2}\right)t^2}=e^{(\mu_1+\mu_2)t+\left(\frac{1}{2}\frac{\sigma_1^2}{n_1}+\frac{1}{2}\frac{\sigma_2^2}{n_2}\right)t^2}\]
	This is the moment-generating function for a normal distribution. \newline \newline
	\boxed{\begin{array}{ccc}\overline{X}_1-\overline{X}_2 \textup{ is a random variable having a normal distribution with} \\ \textup{the mean } \mu_1-\mu_2 \textup{ and the variance } \frac{\sigma^2_1}{n_1}+\frac{\sigma^2_2}{n_2}\end{array}}
	\newpage
	\maketitle HW p.208 \#3,4 \ p. 226 \#43,46 \ p. 239 \#3 \ p. 256 \#66,68,70
	\section[20pt]{p. 256 \#66}
	A random sample of size \(n = 81\) is taken from an infinite population with the mean \(\mu=128\) and the standard deviation \(\sigma=6.3\). With what probability can we assert that the value we obtain for \(\overline{X}\) will not fall between 126.6 and 129.4 if we use \newline
	\textbf{(a)}\ Chebychev's theorem; \newline
	\textbf{(b)}\ the central limit theorem? \newline \newline
	We know that the standard deviation of \(overline{X}=\frac{\sigma}{\sqrt{n}}=\frac{6.3}{9}=0.7\). \newline
	Chebychev's theorem states that
	\[P(|x-\mu|<k\sigma)\geq 1-\frac{1}{k^2}\]
	Thus, by Chebychev's theorem
	\[P(126.6<\overline{x}<129.4)=P(|\overline{x}-128|<2\cdot 0.7)\geq 1-\frac{1}{2^2}\]
	By Theorem 2.3 we have \newline
	\boxed{\textbf{(a)}\ P(\overline{x}<126.6)+P(129.4<\overline{x})\leq 0.2500 \textup{ or } 25.00\%} \newline \newline
	By the central limit theorem, the random variable \(\overline{X}\) approaches a normal distribution with the same mean and variance. \newline
	Referencing Table III, if \(z=2\) then \(P(\overline{x}<126.6)+P(129.4<\overline{x})=1-2(0.4772)=0.0456\).
	\newline
	\boxed{\textbf{(b)}\ P(\overline{x}<126.6)+P(129.4<\overline{x}) \leq 0.0456 \textup{ or } 4.56\%}
	\newpage
	\maketitle HW p.208 \#3,4 \ p. 226 \#43,46 \ p. 239 \#3 \ p. 256 \#66,68,70
	\section[20pt]{p. 256 \#68}
	A random sample of size \(n=225\) is to be taken from an exponential population with \(\theta=4\). Based on the central limit theorem, what is the probability that the mean of the sample will exceed 4.5? \newline \newline
	By Corollary 6.1, the mean and the variance of the population are 4 and 16 respectively. \newline
	Thus, the mean and standard deviation of \(\overline{X}\) are 4 and \(\frac{4}{15}\) respectively. \newline
	\newline
	4.5 is thus 1.875 standard deviations away from the mean.
	\newline
	By the central limit theorem and by 
	referencing Table III, if \(z=1.875\) then \(P(\overline{x}>4.5)=0.5-0.4696=0.0304\).
	\newline \newline
	\boxed{P(\overline{x}>4.5)=0.0304 \textup{ or } 3.04\%}
	\newpage
	\maketitle HW p.208 \#3,4 \ p. 226 \#43,46 \ p. 239 \#3 \ p. 256 \#66,68,70
	\section[20pt]{p. 256 \#70}
	A random sample of size 64 is taken from a normal population with \(\mu=51.4\) and \(\sigma=6.8\). What is the probability that the mean of the sample will \newline
	\textbf{(a)}\ exceed 52.9; \newline
	\textbf{(b)}\ fall between 50.5 and 52.3; \newline
	\textbf{(c)}\ be less than 50.6 \newline \newline
	The mean and standard deviation of \(\overline{X}\) are 51.4 and 0.85 respectively. \newline \newline
	52.9 is therefore 1.7647 standard deviations away from the mean. \newline
	Referring to Table 3 with z = 1.76 we get the value 0.4608. Thus, \(P(\overline{X}>52.9=0.5-0.4608=0.0392\)
	\newline
	\boxed{\textbf{(a)} \textup{The probability the mean of the sample exceeds 52.9 is} 0.0392 \textup{ or }3.92\%}
	\newline
	\newline
	50.5 and 52.3 are both 1.0588 standard deviations away from the mean. \newline
	Referring to Table 3 with z = 1.06 we get the value 0.3554. Thus, \(P(50.5<\overline{X}<52.3=2*0.3554=0.7108\)
	\newline
	\boxed{\textbf{(b)} \textup{The probability the mean of the sample is between 50.5 and 52.3 is} 0.7108 \textup{ or }71.08\%}
	\newline
	\newline
	50.6 is 0.9412 standard deviations from the mean of the sample.
	Referring to Table 3 with z = 0.94 we get the value 0.3264. Thus, \(P(\overline{X}<50.6=0.5-0.3264=0.1736\)
	\newline
	\boxed{\textbf{(c)} \textup{The probability the mean of the sample is less than 50.6 is} 0.1736 \textup{ or }17.36\%}
	\newpage
	\section[20pt]{Poisson-Gamma 4.3-1}
	Telephone calls enter a college switchboard at a mean rate of 2/3 call per minute according to a Poisson process. Let X denote the waiting time until the tenth call arrives. \newline
	\textbf{(a)}\ What is the p.d.f. of X? \newline
	\textbf{(b)}\ What are the moment-generating function, mean, and variance of X?
	\[F_x(x)=P(X\leq x)=1-P(X>x)\]
	By scaling, and by Def. 5.7 we have
	\[F_x(x)=1-\sum_{i=0}^{k-1}\frac{()\lambda x)^i}{i!}e^{-\lambda x}\]
	To get the p.d.f. of X, we differentiate both sides
	\[f(x)=-\sum_{i=0}^{k-1}\frac{i(\lambda x)^{i-1}e^{-\lambda x}-(\lambda x)^{i}e^{-\lambda x}}{i!}\]
	\[f(x)=-e^{-\lambda x}\sum_{i=0}^{k-1}\frac{\lambda ^ix^{i-1}}{(i-1)!}-\frac{\lambda ^{i+1}x^i}{i!}\]
	This is a telescoping series! Thus, we are left with the first part evaluated at the lower bound plus the second part evaluated at the upper bound. Everything else cancels out.
	\[f(x)=-e^{-\lambda x}\left(\lim_{i \rightarrow 0}\frac{\lambda^ix^{i-1}}{(i-1)!}-\frac{\lambda ^k}{(k-1)!}x^{k-1}\right)\]
	\[f(x)=\frac{1}{\left(\frac{1}{\lambda}\right)^k\Gamma(k)}x^{k-1}e^{\frac{-x}{\frac{1}{\lambda}}}\]
	Plugging in \(\frac{2}{3}\) for \(\lambda\) and 10 for k, we have \newline
	\boxed{\textbf{(a)}\ f(x)=\frac{1}{\left(\frac{3}{2}\right)^{10}\Gamma(10)}x^{9}e^{\frac{-3x}{2}},\ x>0} \newline \newline
	This is the p.d.f. of a Gamma random variable with \(\alpha=10,\ \beta = \frac{3}{2}\).
	\newline
	By Theorem 6.3 the mean and variance of the Gamma distribution are \(\mu=\alpha\beta\) and \(\sigma^2=\alpha\beta^2\). \newline \newline
	Plugging in 10 for \(\alpha\) and \(\frac{3}{2}\) for \(\beta\) we have \newline
	\boxed{\textbf{(b)}\ \mu_x=15,\ \sigma^2_x=\frac{45}{2}}
	\newpage
	\section[20pt]{Poisson-Gamma 4.3-6}
	Let X denote the number of alpha particles emitted by barium-133 and observed by a Geiger counter in a fixed position. Assume X has a Poisson distribution and \(\lambda=14.7\) is the mean number of counts per second. Let W denote the waiting time to observe 100 counts. \newline
	Twenty-five independent observations of W are
	\[\begin{matrix}
	6.9 & 7.3 & 6.7 & 6.4 & 6.3 \\
	5.9 & 7.0 & 7.1 & 6.5 & 7.6 \\
	7.2 & 7.1 & 6.1 & 7.3 & 7.6 \\
	7.6 & 6.7 & 6.3 & 5.7 & 6.7 \\
	7.5 & 5.3 & 5.4 & 7.4 & 6.9
	\end{matrix}\]	
	Give the p.d.f., mean, and variance of W.
	\newline \newline
	For this part of the problem, the table of twenty-five observations is unnecessary. \newline
	We know that W must be a Gamma random variable with \(\alpha=100\) and \(\beta=0.068\). \newline
	We can use Theorem 6.3 to find the mean and variance. \newline \newline
	\boxed{\begin{array}{ccc}
			f(w) = \frac{1}{(0.068^{100})99!}w^{99}e^{-14.7w},\ w>0 \\ \mu_w = 6.8,\ \sigma^2_w = 680
		\end{array}}
\end{document}